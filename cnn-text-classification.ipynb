{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "from model import CharacterCNN\n",
    "from model import weights_init\n",
    "from model import DatasetReader\n",
    "\n",
    "# Set random seem for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "th.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Character vocab\n",
    "char_inventory = 'abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:\\'\"/\\\\|_@#$%ˆ&*˜`+-=<>()[]{}\\n'\n",
    "\n",
    "vocab = {c:i for i,c in enumerate(char_inventory)}\n",
    "\n",
    "# One hot embedding size\n",
    "nchars = len(char_inventory)\n",
    "\n",
    "#Input length\n",
    "input_length = 1014\n",
    "\n",
    "# Decide which device we want to run on\n",
    "ngpu = 1\n",
    "device = th.device(\"cuda:0\" if (th.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "#Batch size\n",
    "batch_size = 128\n",
    "\n",
    "#Number of threads for the data loader\n",
    "workers = 2\n",
    "\n",
    "#Number of epochs\n",
    "nepochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CharacterCNN(nchars).to(device)\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "cnn.apply(weights_init)\n",
    "cnn.train()\n",
    "# Print the model\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DatasetReader('./data/yelp_review_full_csv/train.csv', vocab, input_length, nchars)\n",
    "dataloader = th.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The objective\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#Optimizer\n",
    "optimizer = optim.Adam(cnn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "lr = 0.01\n",
    "\n",
    "#Main training loop\n",
    "for epoch in range(nepochs):\n",
    "        \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        inputs = batch[0].type(th.FloatTensor).to(device)\n",
    "        targets = batch[1].type(th.LongTensor).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = cnn(inputs)\n",
    "        loss = criterion(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Output training stats\n",
    "        if i % 5 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss: %.4f'\n",
    "                  % (epoch, nepochs, i, len(dataloader), loss.item()))\n",
    "            losses.append(loss)\n",
    "        if i % 100 == 0:\n",
    "            th.save({\n",
    "                'model': cnn.state_dict(),\n",
    "                'epoch': epoch,\n",
    "            }, './output/cnn.pth')\n",
    "th.save({\n",
    "    'model': cnn.state_dict(),\n",
    "    'epoch': epoch,\n",
    "}, './output/cnn.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = DatasetReader('./data/yelp_review_full_csv/test.csv', vocab, input_length, nchars)\n",
    "dataloader = th.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.load_state_dict(th.load('./output/cnn.pth')['model'])\n",
    "cnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "gt = []\n",
    "for batch in tqdm(dataloader):\n",
    "    inputs = batch[0].type(th.FloatTensor).to(device)\n",
    "    targets = batch[1].type(th.LongTensor).to(device)\n",
    "        \n",
    "    out = cnn(inputs)\n",
    "    preds.extend(list(np.argmax(out.cpu().data.numpy(), axis=1)))\n",
    "    gt.extend(list(targets.squeeze().cpu().data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_rate = sum([preds[i] != gt[i] for i in range(len(gt))] ) / len(gt)\n",
    "print(\"Error rate: %.4f\"%err_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jhubMachineLearning",
   "language": "python",
   "name": "jhubmachinelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
